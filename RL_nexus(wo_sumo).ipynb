{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctk0zQxl8Gbp",
        "outputId": "058ab853-7ae5-4c75-a1cc-528323129269"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision torchaudio gymnasium pandas numpy matplotlib tqdm mpmath==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LsUxHnp3TzC"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "KQWOqpAw3pAl",
        "outputId": "d88c882d-20d3-414a-b112-f1836b4ea2dc"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"train_data.csv\")\n",
        "\n",
        "# Drop timestamp or non-numeric columns if they exist\n",
        "if 'timestamp' in data.columns:\n",
        "    data = data.drop(columns=['timestamp'])\n",
        "\n",
        "# Ensure numeric and fill missing values\n",
        "data = data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "print(\"Shape:\", data.shape)\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj03-cHR3t81"
      },
      "outputs": [],
      "source": [
        "class TrafficEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(TrafficEnv, self).__init__()\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.max_index = len(data) - 1\n",
        "\n",
        "        # Define spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
        "                                            shape=(len(data.columns)-1,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(4)  # 4 signal phases\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        # Start from a random point in dataset\n",
        "        self.current_step = np.random.randint(0, self.max_index - 50)\n",
        "        obs = self.data.iloc[self.current_step, :-1].values.astype(np.float32)\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_index\n",
        "\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        wait_time = row.get('waiting_time', 0)\n",
        "        queue_len = row.get('queue_length', 0)\n",
        "        emergency = row.get('emergency_detected', 0) if 'emergency_detected' in row else 0\n",
        "\n",
        "        reward = self.compute_reward(wait_time, queue_len, emergency, action)\n",
        "        next_obs = row[:-1].values.astype(np.float32)\n",
        "        truncated = False\n",
        "        info = {}\n",
        "\n",
        "        return next_obs, reward, done, truncated, info\n",
        "\n",
        "    def compute_reward(self, wait_time, queue_len, emergency_detected, action):\n",
        "        reward = - (0.7 * wait_time + 0.3 * queue_len)\n",
        "        if emergency_detected and action == 0:  # example: NS-green helps emergency\n",
        "            reward += 20\n",
        "        return reward\n",
        "\n",
        "    def render(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5tJRLcWVDXil",
        "outputId": "ce461ce6-90ea-4c38-a18d-0167e42ea5d3"
      },
      "outputs": [],
      "source": [
        "'''def compute_reward(wait_time, queue_len, emergency_detected):\n",
        "\n",
        "    reward = - (0.7 * wait_time + 0.3 * queue_len)\n",
        "    if emergency_detected:\n",
        "        reward += 20  # positive reward for prioritizing emergency\n",
        "    return reward'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_RYzK7iDZ84"
      },
      "outputs": [],
      "source": [
        "class PPOAgent(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4):\n",
        "        super(PPOAgent, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state)\n",
        "        probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action)\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        return log_prob, self.critic(state), entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPjzY0Ou4BQC",
        "outputId": "59877ea3-980f-4aeb-9dd3-8af51bbdee8a"
      },
      "outputs": [],
      "source": [
        "env = TrafficEnv(data)\n",
        "agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "def train_ppo(episodes=200, gamma=0.99, clip_epsilon=0.2):\n",
        "    reward_history = []\n",
        "\n",
        "    for episode in tqdm(range(episodes)):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = agent.act(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "            # PPO updates\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            value = agent.critic(state_tensor)\n",
        "            next_value = agent.critic(next_state_tensor)\n",
        "            advantage = reward_tensor + gamma * next_value - value\n",
        "\n",
        "            new_log_prob, _, entropy = agent.evaluate(\n",
        "                state_tensor, torch.tensor([action], dtype=torch.long)\n",
        "            )\n",
        "\n",
        "            ratio = (new_log_prob - log_prob).exp()\n",
        "            surr1 = ratio * advantage.detach()\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage.detach()\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * advantage.pow(2) - 0.01 * entropy\n",
        "\n",
        "            agent.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            agent.optimizer.step()\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        reward_history.append(total_reward)\n",
        "        print(f\"Episode {episode+1} | Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    return reward_history\n",
        "\n",
        "rewards = train_ppo(episodes=200)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZIsvY0-uF7V",
        "outputId": "01ad29ad-bb4d-498c-ab3c-281486167c7e"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model.pt\")\n",
        "print(\"Model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "U3PxHWqJ4M4P",
        "outputId": "49e642a9-4fa3-4a2a-b421-4a14a73a032b"
      },
      "outputs": [],
      "source": [
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"PPO Training Progress for Traffic Signal Control\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "addgK9uA96i-",
        "outputId": "63ab9943-e5d1-48d1-ed2e-9468a2810c28"
      },
      "outputs": [],
      "source": [
        "!pip install stable_baselines3\n",
        "\n",
        "# Re-initialize the PPOAgent model structure\n",
        "loaded_agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "# Load the state dictionary into the agent\n",
        "loaded_agent.load_state_dict(torch.load(\"/content/ppo_traffic_model.pt\"))\n",
        "loaded_agent.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7631741",
        "outputId": "ee649d85-cd16-4d0f-fcba-532adbdc2900"
      },
      "outputs": [],
      "source": [
        "print(\"Continuing training for 200 more episodes...\")\n",
        "new_rewards = train_ppo(episodes=200)\n",
        "rewards.extend(new_rewards)\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1e6d94e"
      },
      "outputs": [],
      "source": [
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"PPO Training Progress for Traffic Signal Control (Extended)\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
