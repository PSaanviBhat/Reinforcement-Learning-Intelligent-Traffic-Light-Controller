{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctk0zQxl8Gbp",
        "outputId": "aef145d5-541b-45e3-dba5-d073fa98ce66"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision torchaudio gymnasium pandas numpy matplotlib tqdm mpmath==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LsUxHnp3TzC"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "KQWOqpAw3pAl",
        "outputId": "8659b530-147c-4822-9bf6-f50a99c29f84"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"train_data.csv\")\n",
        "\n",
        "# Drop timestamp or non-numeric columns if they exist\n",
        "if 'timestamp' in data.columns:\n",
        "    data = data.drop(columns=['timestamp'])\n",
        "\n",
        "# Ensure numeric and fill missing values\n",
        "data = data.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "print(\"Shape:\", data.shape)\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj03-cHR3t81"
      },
      "outputs": [],
      "source": [
        "class TrafficEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(TrafficEnv, self).__init__()\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.max_index = len(data) - 1\n",
        "\n",
        "        # Define spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
        "                                            shape=(len(data.columns)-1,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(4)  # 4 signal phases\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        # Start from a random point in dataset\n",
        "        self.current_step = np.random.randint(0, self.max_index - 50)\n",
        "        obs = self.data.iloc[self.current_step, :-1].values.astype(np.float32)\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_index\n",
        "\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        wait_time = row.get('waiting_time', 0)\n",
        "        queue_len = row.get('queue_length', 0)\n",
        "        emergency = row.get('emergency_detected', 0) if 'emergency_detected' in row else 0\n",
        "\n",
        "        reward = self.compute_reward(wait_time, queue_len, emergency, action)\n",
        "        next_obs = row[:-1].values.astype(np.float32)\n",
        "        truncated = False\n",
        "        info = {}\n",
        "\n",
        "        return next_obs, reward, done, truncated, info\n",
        "\n",
        "    def compute_reward(self, wait_time, queue_len, emergency_detected, action):\n",
        "        reward = - (0.7 * wait_time + 0.3 * queue_len)\n",
        "        if emergency_detected and action == 0:  # example: NS-green helps emergency\n",
        "            reward += 20\n",
        "        return reward\n",
        "\n",
        "    def render(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tJRLcWVDXil"
      },
      "outputs": [],
      "source": [
        "'''def compute_reward(wait_time, queue_len, emergency_detected):\n",
        "\n",
        "    reward = - (0.7 * wait_time + 0.3 * queue_len)\n",
        "    if emergency_detected:\n",
        "        reward += 20  # positive reward for prioritizing emergency\n",
        "    return reward'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_RYzK7iDZ84"
      },
      "outputs": [],
      "source": [
        "class PPOAgent(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4):\n",
        "        super(PPOAgent, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state)\n",
        "        probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action)\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        probs = self.actor(state)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        return log_prob, self.critic(state), entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "bPjzY0Ou4BQC",
        "outputId": "9aa4253b-b6c3-40c8-bfb0-2e8369599621"
      },
      "outputs": [],
      "source": [
        "env = TrafficEnv(data)\n",
        "agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_ppo(\n",
        "    episodes=200,\n",
        "    gamma=0.99,\n",
        "    clip_epsilon=0.2,\n",
        "    save_path=\"ppo_traffic_model_latest.pt\",\n",
        "    rewards_path=\"reward_log.json\"\n",
        "):\n",
        "    reward_history = []\n",
        "\n",
        "    # Resume from saved rewards if file exists\n",
        "    if os.path.exists(rewards_path):\n",
        "        with open(rewards_path, \"r\") as f:\n",
        "            reward_history = json.load(f)\n",
        "        print(f\"Loaded {len(reward_history)} previous rewards from '{rewards_path}'\")\n",
        "\n",
        "    # Continue training for new episodes\n",
        "    for episode in tqdm(range(len(reward_history), len(reward_history) + episodes)):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = agent.act(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "            # PPO update\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            value = agent.critic(state_tensor)\n",
        "            next_value = agent.critic(next_state_tensor)\n",
        "            advantage = reward_tensor + gamma * next_value - value\n",
        "\n",
        "            new_log_prob, _, entropy = agent.evaluate(\n",
        "                state_tensor, torch.tensor([action], dtype=torch.long)\n",
        "            )\n",
        "\n",
        "            ratio = (new_log_prob - log_prob).exp()\n",
        "            surr1 = ratio * advantage.detach()\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage.detach()\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * advantage.pow(2) - 0.01 * entropy\n",
        "\n",
        "            agent.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            agent.optimizer.step()\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        reward_history.append(total_reward)\n",
        "        print(f\"Episode {episode + 1} | Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "        # Save model & rewards every 10 episodes\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            torch.save(agent.state_dict(), save_path)\n",
        "            with open(rewards_path, \"w\") as f:\n",
        "                json.dump(reward_history, f)\n",
        "            print(f\"üíæ Progress saved ‚Äî {episode + 1} episodes complete\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    torch.save(agent.state_dict(), save_path)\n",
        "    with open(rewards_path, \"w\") as f:\n",
        "        json.dump(reward_history, f)\n",
        "\n",
        "    return reward_history\n",
        "\n",
        "\n",
        "rewards = train_ppo(episodes=500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZIsvY0-uF7V",
        "outputId": "a43be161-8bba-4db1-89ec-1da7ef571cb5"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model_500.pt\")\n",
        "print(\"Model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YKSSiDt__MAS",
        "outputId": "5f53c8a4-5a8a-4652-b67f-464e426f68a4"
      },
      "outputs": [],
      "source": [
        "print(\"Models saved: ppo_traffic_model_latest.pt, reward_log.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "U3PxHWqJ4M4P",
        "outputId": "04325d1c-23a2-4314-859c-d4e779b09106"
      },
      "outputs": [],
      "source": [
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"PPO Training Progress for Traffic Signal Control\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "yonfbcB6A5CW",
        "outputId": "6a91ebf3-b4a6-4e2a-9db0-b8b95936bc2f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "plt.plot(pd.Series(rewards).rolling(20).mean(), color='blue', linewidth=2)\n",
        "plt.title(\"Smoothed Rewards (20-episode Moving Average)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward (Smoothed)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "addgK9uA96i-",
        "outputId": "c9e0d133-831e-4c26-d34d-108f19841fa3"
      },
      "outputs": [],
      "source": [
        "!pip install stable_baselines3\n",
        "\n",
        "# Re-initialize the PPOAgent model structure\n",
        "loaded_agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "# Load the state dictionary into the agent\n",
        "loaded_agent.load_state_dict(torch.load(\"ppo_traffic_model_500.pt\"))\n",
        "loaded_agent.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgwUCekY4ktF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7631741",
        "outputId": "a081f76c-0a70-4357-c029-fbf02a116b91"
      },
      "outputs": [],
      "source": [
        "print(\"Continuing training for 500 more episodes...\")\n",
        "new_rewards = train_ppo(episodes=500)\n",
        "rewards.extend(new_rewards)\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "QvP8mY5IF1GZ",
        "outputId": "1c85be12-74e6-480d-dd58-19a309dab385"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "plt.plot(pd.Series(rewards).rolling(20).mean(), color='blue', linewidth=2)\n",
        "plt.title(\"Smoothed Rewards (20-episode Moving Average)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward (Smoothed)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gr7mzqCUkrl"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model_1000.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1H-9482HGc4D",
        "outputId": "8f2a786f-bac3-4900-fa48-60a56f29344c"
      },
      "outputs": [],
      "source": [
        "print(\"Models saved: ppo_traffic_model_latest.pt, reward_log.json, ppo_traffic_model_1000.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1e6d94e"
      },
      "outputs": [],
      "source": [
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"PPO Training Progress for Traffic Signal Control (Extended)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap-4SfmD35sy",
        "outputId": "7e058ba5-d57d-4309-a9ac-cd5c7ec374af"
      },
      "outputs": [],
      "source": [
        "!pip install stable_baselines3\n",
        "\n",
        "# Re-initialize the PPOAgent model structure\n",
        "loaded_agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "# Load the state dictionary into the agent\n",
        "loaded_agent.load_state_dict(torch.load(\"ppo_traffic_model_1000.pt\"))\n",
        "loaded_agent.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXWzXt3A4t4j"
      },
      "outputs": [],
      "source": [
        "env = TrafficEnv(data)\n",
        "agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_ppo(\n",
        "    episodes=200,\n",
        "    gamma=0.99,\n",
        "    clip_epsilon=0.2,\n",
        "    save_path=\"ppo_traffic_model_latest.pt\",\n",
        "    rewards_path=\"reward_log.json\"\n",
        "):\n",
        "    reward_history = []\n",
        "\n",
        "    # Resume from saved rewards if file exists\n",
        "    if os.path.exists(rewards_path):\n",
        "        with open(rewards_path, \"r\") as f:\n",
        "            reward_history = json.load(f)\n",
        "        print(f\"‚úÖ Loaded {len(reward_history)} previous rewards from '{rewards_path}'\")\n",
        "\n",
        "    # Continue training for new episodes\n",
        "    for episode in tqdm(range(len(reward_history), len(reward_history) + episodes)):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = agent.act(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "            # PPO update\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            value = agent.critic(state_tensor)\n",
        "            next_value = agent.critic(next_state_tensor)\n",
        "            advantage = reward_tensor + gamma * next_value - value\n",
        "\n",
        "            new_log_prob, _, entropy = agent.evaluate(\n",
        "                state_tensor, torch.tensor([action], dtype=torch.long)\n",
        "            )\n",
        "\n",
        "            ratio = (new_log_prob - log_prob).exp()\n",
        "            surr1 = ratio * advantage.detach()\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage.detach()\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * advantage.pow(2) - 0.01 * entropy\n",
        "\n",
        "            agent.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            agent.optimizer.step()\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        reward_history.append(total_reward)\n",
        "        print(f\"Episode {episode + 1} | Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "        # Save model & rewards every 10 episodes\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            torch.save(agent.state_dict(), save_path)\n",
        "            with open(rewards_path, \"w\") as f:\n",
        "                json.dump(reward_history, f)\n",
        "            print(f\"üíæ Progress saved ‚Äî {episode + 1} episodes complete\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    torch.save(agent.state_dict(), save_path)\n",
        "    with open(rewards_path, \"w\") as f:\n",
        "        json.dump(reward_history, f)\n",
        "\n",
        "    return reward_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI_kiRwbUpOw"
      },
      "outputs": [],
      "source": [
        "agent.load_state_dict(torch.load(\"ppo_traffic_model_1000.pt\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STQ7_oQvX268"
      },
      "outputs": [],
      "source": [
        "''' rewards: rewards+ new_rewards+rewards_more'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OOHd-LnTVP_e",
        "outputId": "a9e661a7-79ef-4746-b5b4-5bd5e3b6be87"
      },
      "outputs": [],
      "source": [
        "print(\"Continuing training for 500 more episodes...\")\n",
        "new_rewards = train_ppo(episodes=500)\n",
        "rewards.extend(new_rewards)\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MByPFy9FZDPu"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model_1500.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XG4UCrL4Fj3j",
        "outputId": "24086235-181d-41d2-c19d-a67b0257c84c"
      },
      "outputs": [],
      "source": [
        "print(\"Models saved: ppo_traffic_model_latest.pt, reward_log.json, ppo_traffic_model_1500.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "xSlJVlg-Fvxh",
        "outputId": "0aa78491-89f2-4949-d9e9-89c50c5528f0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "plt.plot(pd.Series(new_rewards).rolling(20).mean(), color='blue', linewidth=2)\n",
        "plt.title(\"Smoothed Rewards (20-episode Moving Average)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward (Smoothed)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiU4iTp6ajIK"
      },
      "outputs": [],
      "source": [
        "#all_rewards=rewards+ new_rewards+rewards_more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK2JXEe-arL1"
      },
      "outputs": [],
      "source": [
        "plt.plot(rewards)\n",
        "plt.title(\"Raw Rewards (Noisy)-200\")\n",
        "plt.plot(all_rewards)\n",
        "plt.title(\"Raw Rewards (Noisy)-600\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RbEuNmya5_e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "plt.plot(pd.Series(rewards).rolling(20).mean(), color='blue', linewidth=2)\n",
        "plt.title(\"Smoothed Rewards (20-Episode Moving Average)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZsUew16filh",
        "outputId": "f73a8c23-61ad-4ad9-fb51-9c335d3b2249"
      },
      "outputs": [],
      "source": [
        "!pip install stable_baselines3\n",
        "\n",
        "# Re-initialize the PPOAgent model structure\n",
        "loaded_agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "# Load the state dictionary into the agent\n",
        "loaded_agent.load_state_dict(torch.load(\"ppo_traffic_model_1500.pt\"))\n",
        "loaded_agent.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9588A2DCfmnl"
      },
      "outputs": [],
      "source": [
        "env = TrafficEnv(data)\n",
        "agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzZ3uvUpgoAN",
        "outputId": "8054b983-bf5d-428d-c4f9-aed89ae6dc5b"
      },
      "outputs": [],
      "source": [
        "agent.load_state_dict(torch.load(\"ppo_traffic_model_1500.pt\"))\n",
        "print(\"Loaded model from 1500-episode checkpoint\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLyV5zKkgtWf",
        "outputId": "315314f6-04bb-4ac1-a2a2-cda08c68aa9f"
      },
      "outputs": [],
      "source": [
        "print(\"Current learning rate:\")\n",
        "for g in agent.optimizer.param_groups:\n",
        "    print(g['lr'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVJOnZUrgysZ",
        "outputId": "e7ec23a1-e552-4354-e4a4-b6b895994b5b"
      },
      "outputs": [],
      "source": [
        "for g in agent.optimizer.param_groups:\n",
        "    g['lr'] = 1e-4  # or 5e-5 if training is still noisy\n",
        "\n",
        "print(\"Learning rate successfully changed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwY7j_ONg2aJ",
        "outputId": "79c849c9-9fc2-4872-a62e-67100347963c"
      },
      "outputs": [],
      "source": [
        "print(\"Current learning rate:\")\n",
        "for g in agent.optimizer.param_groups:\n",
        "    print(g['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaETajzchSK0"
      },
      "outputs": [],
      "source": [
        "import os # Add this line\n",
        "import json # Add this line if not already imported\n",
        "\n",
        "def train_ppo(\n",
        "    episodes=200,\n",
        "    gamma=0.99,\n",
        "    clip_epsilon=0.2,\n",
        "    save_path=\"ppo_traffic_model_latest.pt\",\n",
        "    rewards_path=\"reward_log.json\"\n",
        "):\n",
        "    reward_history = []\n",
        "\n",
        "    # Resume from saved rewards if file exists\n",
        "    if os.path.exists(rewards_path):\n",
        "        with open(rewards_path, \"r\") as f:\n",
        "            reward_history = json.load(f)\n",
        "        print(f\"‚úÖ Loaded {len(reward_history)} previous rewards from '{rewards_path}'\")\n",
        "\n",
        "    # Continue training for new episodes\n",
        "    for episode in tqdm(range(len(reward_history), len(reward_history) + episodes)):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = agent.act(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "            # PPO update\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            value = agent.critic(state_tensor)\n",
        "            next_value = agent.critic(next_state_tensor)\n",
        "            advantage = reward_tensor + gamma * next_value - value\n",
        "\n",
        "            new_log_prob, _, entropy = agent.evaluate(\n",
        "                state_tensor, torch.tensor([action], dtype=torch.long)\n",
        "            )\n",
        "\n",
        "            ratio = (new_log_prob - log_prob).exp()\n",
        "            surr1 = ratio * advantage.detach()\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage.detach()\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * advantage.pow(2) - 0.01 * entropy\n",
        "\n",
        "            agent.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            agent.optimizer.step()\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        reward_history.append(total_reward)\n",
        "        print(f\"Episode {episode + 1} | Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "        # Save model & rewards every 10 episodes\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            torch.save(agent.state_dict(), save_path)\n",
        "            with open(rewards_path, \"w\") as f:\n",
        "                json.dump(reward_history, f)\n",
        "            print(f\"üíæ Progress saved ‚Äî {episode + 1} episodes complete\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    torch.save(agent.state_dict(), save_path)\n",
        "    with open(rewards_path, \"w\") as f:\n",
        "        json.dump(reward_history, f)\n",
        "\n",
        "    return reward_history  # <- ‚úÖ should be on its own line!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6290MmakBt6"
      },
      "outputs": [],
      "source": [
        "# Training with new Learning Rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vNWd8YbPhVd-",
        "outputId": "c92ddca9-8caa-4a00-eca5-ddef66ba5172"
      },
      "outputs": [],
      "source": [
        "print(\"Continuing training for 500 more episodes with DIFFERENT learning rate:\")\n",
        "newlr_rewards = train_ppo(episodes=500)\n",
        "rewards.extend(newlr_rewards)\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-z3QQcvn0io"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model_2000.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gRtJvPEsnncc",
        "outputId": "ae75aa20-947b-4a5a-b142-c11c98e71915"
      },
      "outputs": [],
      "source": [
        "print(\"Models saved: ppo_traffic_model_latest.pt, reward_log.json, ppo_traffic_model_2000.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "t977Ea82n75v",
        "outputId": "60866983-0472-4559-91ba-83dd8e634969"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "plt.plot(pd.Series(newlr_rewards).rolling(20).mean(), color='blue', linewidth=2)\n",
        "plt.title(\"Smoothed Rewards (20-episode Moving Average)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward (Smoothed)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WbSWm85tqCO8",
        "outputId": "e5545cf9-81b4-44b5-957b-c649545b7797"
      },
      "outputs": [],
      "source": [
        "print(\"Continuing training for 500 more episodes with DIFFERENT learning rate:\")\n",
        "newlr2_rewards = train_ppo(episodes=500)\n",
        "rewards.extend(newlr2_rewards)\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3hxKZ06sYf1"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model_2500.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_uKIC54GsUKu",
        "outputId": "88c3cac5-6bc3-4f69-daa9-ab06684a9d69"
      },
      "outputs": [],
      "source": [
        "print(\"Models saved: ppo_traffic_model_latest.pt, reward_log.json, ppo_traffic_model_2500.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "8ghAyIzev7It",
        "outputId": "a083245a-2035-471a-f187-4716a2d8e69e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "plt.plot(pd.Series(newlr2_rewards).rolling(20).mean(), color='blue', linewidth=2)\n",
        "plt.title(\"Smoothed Rewards (20-episode Moving Average)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward (Smoothed)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1DY4GuOyv4a",
        "outputId": "379836a3-7a67-49f9-e3e9-32876e1980ef"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import json, os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "env = TrafficEnv(data)\n",
        "\n",
        "# Load agent with same architecture\n",
        "agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "# Load previous trained weights if available\n",
        "model_path = \"/content/ppo_traffic_model_2500.pt\"\n",
        "if os.path.exists(model_path):\n",
        "    agent.load_state_dict(torch.load(model_path))\n",
        "    print(\"‚úî Loaded existing trained model:\", model_path)\n",
        "else:\n",
        "    print(\"‚ö† No previous model found ‚Äî training will start fresh.\")\n",
        "\n",
        "\n",
        "# 2Ô∏è‚É£ PPO Evaluation function (NO EXPLORATION)\n",
        "def evaluate(agent, env, episodes=10):\n",
        "    rewards = []\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        total = 0\n",
        "        while not done:\n",
        "            a, _ = agent.act(s)   # greedy action (actor takes highest prob)\n",
        "            s, r, done, truncated, _ = env.step(a)\n",
        "            total += r\n",
        "        rewards.append(total)\n",
        "    return np.mean(rewards)\n",
        "\n",
        "\n",
        "# 3Ô∏è‚É£ TRAINING FUNCTION (NEW)\n",
        "def train_ppo_next_phase(\n",
        "    episodes=1000,\n",
        "    gamma=0.99,\n",
        "    clip_epsilon=0.2,\n",
        "    save_path=\"ppo_traffic_model_latest.pt\",\n",
        "    rewards_path=\"reward_log.json\"\n",
        "):\n",
        "    # Load previous reward log\n",
        "    reward_history = []\n",
        "    if os.path.exists(rewards_path):\n",
        "        with open(rewards_path, \"r\") as f:\n",
        "            reward_history = json.load(f)\n",
        "        print(f\"‚úî Loaded {len(reward_history)} previous reward entries.\")\n",
        "\n",
        "    # LR scheduler ‚Äî reduces LR when improvement plateaus\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        agent.optimizer,\n",
        "        mode='max',\n",
        "        factor=0.5,     # Halve the LR\n",
        "        patience=5,     # Wait 5 eval cycles\n",
        "        min_lr=1e-6\n",
        "        # verbose=True  # Removed verbose argument\n",
        "    )\n",
        "\n",
        "    print(\"\\n Starting Next-Phase Training...\\n\")\n",
        "\n",
        "    for ep in tqdm(range(episodes)):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = agent.act(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            value = agent.critic(state_tensor)\n",
        "            next_value = agent.critic(next_state_tensor)\n",
        "            advantage = reward_tensor + gamma * next_value - value\n",
        "\n",
        "            new_log_prob, _, entropy = agent.evaluate(\n",
        "                state_tensor, torch.tensor([action], dtype=torch.long)\n",
        "            )\n",
        "\n",
        "            ratio = (new_log_prob - log_prob).exp()\n",
        "            surr1 = ratio * advantage.detach()\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage.detach()\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * advantage.pow(2) - 0.01 * entropy\n",
        "\n",
        "            agent.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            agent.optimizer.step()\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        reward_history.append(total_reward)\n",
        "        print(f\"Episode {len(reward_history)} | Reward: {total_reward:.2f}\")\n",
        "\n",
        "        # Every 100 episodes ‚Üí evaluate & adjust LR\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            eval_reward = evaluate(agent, env, episodes=5)\n",
        "            print(f\"üß™ Evaluation Reward after {len(reward_history)} eps: {eval_reward:.2f}\")\n",
        "\n",
        "            scheduler.step(eval_reward)\n",
        "\n",
        "            # Save progress\n",
        "            torch.save(agent.state_dict(), save_path)\n",
        "            with open(rewards_path, \"w\") as f:\n",
        "                json.dump(reward_history, f)\n",
        "            print(\"üíæ Autosaved model + rewards.\")\n",
        "\n",
        "    # final save\n",
        "    torch.save(agent.state_dict(), save_path)\n",
        "    with open(rewards_path, \"w\") as f:\n",
        "        json.dump(reward_history, f)\n",
        "\n",
        "    print(\"\\nüéâ Training Phase Complete!\")\n",
        "    return reward_history\n",
        "\n",
        "\n",
        "# 4Ô∏è‚É£ Run training\n",
        "new_rewards = train_ppo_next_phase(episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijs5CQ-69cQX"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model_3500.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7WTVvF3l9lB9",
        "outputId": "6dd4f272-736e-4381-c676-3d09f6b8a9a3"
      },
      "outputs": [],
      "source": [
        "print(\"Models saved: ppo_traffic_model_latest.pt, reward_log.json, ppo_traffic_model_3500.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ITB2cQL69qtf",
        "outputId": "3ceb4b0b-657e-432e-dd18-fd9a37e5025f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "plt.plot(pd.Series(newlr2_rewards).rolling(20).mean(), color='blue', linewidth=2)\n",
        "plt.title(\"Smoothed Rewards (20-episode Moving Average)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward (Smoothed)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvdPXtYfRdnm"
      },
      "outputs": [],
      "source": [
        "# TUNING the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHG-NR1pa75a"
      },
      "outputs": [],
      "source": [
        "class TrafficEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(TrafficEnv, self).__init__()\n",
        "        self.data = data.reset_index(drop=True)\n",
        "        self.max_index = len(data) - 1\n",
        "\n",
        "        # Define spaces\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
        "                                            shape=(len(data.columns)-1,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(4)  # 4 signal phases\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        # Start from a random point in dataset\n",
        "        self.current_step = np.random.randint(0, self.max_index - 50)\n",
        "        obs = self.data.iloc[self.current_step, :-1].values.astype(np.float32)\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= self.max_index\n",
        "\n",
        "        row = self.data.iloc[self.current_step]\n",
        "        wait_time = row.get('waiting_time', 0)\n",
        "        queue_len = row.get('queue_length', 0)\n",
        "        emergency = row.get('emergency_detected', 0) if 'emergency_detected' in row else 0\n",
        "\n",
        "        reward = self.compute_reward(wait_time, queue_len, emergency, action)\n",
        "        next_obs = row[:-1].values.astype(np.float32)\n",
        "        truncated = False\n",
        "        info = {}\n",
        "\n",
        "        return next_obs, reward, done, truncated, info\n",
        "\n",
        "    def compute_reward(self, wait_time, queue_len, emergency_detected, action):\n",
        "        wait_time = wait_time / 100\n",
        "        queue_len = queue_len / 50\n",
        "        reward = - (0.6 * wait_time + 0.4 * queue_len)\n",
        "        if emergency_detected:\n",
        "          reward += 10\n",
        "        return reward\n",
        "        '''\n",
        "        reward = - (0.7 * wait_time + 0.3 * queue_len)\n",
        "        if emergency_detected and action == 0:  # example: NS-green helps emergency\n",
        "            reward += 20\n",
        "        return reward'''\n",
        "\n",
        "    def render(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxVGPirZbY14"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilmauhZvcWOn",
        "outputId": "ff9b1ba0-fba7-4e4d-ea91-bae1029b6976"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import json, os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "env = TrafficEnv(data)\n",
        "\n",
        "# Load agent with same architecture\n",
        "agent = PPOAgent(state_dim=len(data.columns)-1, action_dim=4)\n",
        "\n",
        "# Load previous trained weights if available\n",
        "model_path = \"/content/ppo_traffic_model_3500.pt\"\n",
        "if os.path.exists(model_path):\n",
        "    agent.load_state_dict(torch.load(model_path))\n",
        "    print(\"‚úî Loaded existing trained model:\", model_path)\n",
        "else:\n",
        "    print(\"‚ö† No previous model found ‚Äî training will start fresh.\")\n",
        "\n",
        "\n",
        "# 2Ô∏è‚É£ PPO Evaluation function (NO EXPLORATION)\n",
        "def evaluate(agent, env, episodes=10):\n",
        "    rewards = []\n",
        "    for _ in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        total = 0\n",
        "        while not done:\n",
        "            a, _ = agent.act(s)   # greedy action (actor takes highest prob)\n",
        "            s, r, done, truncated, _ = env.step(a)\n",
        "            total += r\n",
        "        rewards.append(total)\n",
        "    return np.mean(rewards)\n",
        "\n",
        "\n",
        "# 3Ô∏è‚É£ TRAINING FUNCTION (NEW)\n",
        "def train_ppo_next_phase(\n",
        "    episodes=1000,\n",
        "    gamma=0.99,\n",
        "    clip_epsilon=0.2,\n",
        "    save_path=\"ppo_traffic_model_latest.pt\",\n",
        "    rewards_path=\"reward_log.json\"\n",
        "):\n",
        "    # Load previous reward log\n",
        "    reward_history = []\n",
        "    if os.path.exists(rewards_path):\n",
        "        with open(rewards_path, \"r\") as f:\n",
        "            reward_history = json.load(f)\n",
        "        print(f\"‚úî Loaded {len(reward_history)} previous reward entries.\")\n",
        "\n",
        "    # LR scheduler ‚Äî reduces LR when improvement plateaus\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        agent.optimizer,\n",
        "        mode='max',\n",
        "        factor=0.5,     # Halve the LR\n",
        "        patience=5,     # Wait 5 eval cycles\n",
        "        min_lr=1e-6\n",
        "        # verbose=True  # Removed verbose argument\n",
        "    )\n",
        "\n",
        "    print(\"\\n Starting Next-Phase Training...\\n\")\n",
        "\n",
        "    for ep in tqdm(range(episodes)):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action, log_prob = agent.act(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            value = agent.critic(state_tensor)\n",
        "            next_value = agent.critic(next_state_tensor)\n",
        "            advantage = reward_tensor + gamma * next_value - value\n",
        "\n",
        "            new_log_prob, _, entropy = agent.evaluate(\n",
        "                state_tensor, torch.tensor([action], dtype=torch.long)\n",
        "            )\n",
        "\n",
        "            ratio = (new_log_prob - log_prob).exp()\n",
        "            surr1 = ratio * advantage.detach()\n",
        "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage.detach()\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * advantage.pow(2) - 0.01 * entropy\n",
        "\n",
        "            agent.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            agent.optimizer.step()\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        reward_history.append(total_reward)\n",
        "        print(f\"Episode {len(reward_history)} | Reward: {total_reward:.2f}\")\n",
        "\n",
        "        # Every 100 episodes ‚Üí evaluate & adjust LR\n",
        "        if (ep + 1) % 100 == 0:\n",
        "            eval_reward = evaluate(agent, env, episodes=5)\n",
        "            print(f\"üß™ Evaluation Reward after {len(reward_history)} eps: {eval_reward:.2f}\")\n",
        "\n",
        "            scheduler.step(eval_reward)\n",
        "\n",
        "            # Save progress\n",
        "            torch.save(agent.state_dict(), save_path)\n",
        "            with open(rewards_path, \"w\") as f:\n",
        "                json.dump(reward_history, f)\n",
        "            print(\"üíæ Autosaved model + rewards.\")\n",
        "\n",
        "    # final save\n",
        "    torch.save(agent.state_dict(), save_path)\n",
        "    with open(rewards_path, \"w\") as f:\n",
        "        json.dump(reward_history, f)\n",
        "\n",
        "    print(\"\\nüéâ Training Phase Complete!\")\n",
        "    return reward_history\n",
        "\n",
        "\n",
        "# 4Ô∏è‚É£ Run training\n",
        "new_rewards = train_ppo_next_phase(episodes=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf4bIcq_hozH"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model_4000.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EkhaYu34hXQK",
        "outputId": "6df8442f-a8a0-4bf0-9bab-a3b5ef6e87ee"
      },
      "outputs": [],
      "source": [
        "print(\"Models saved: ppo_traffic_model_latest.pt, reward_log.json, ppo_traffic_model_4000.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Zsmjp1MViQhp",
        "outputId": "3eedf3e8-2757-40a5-a97e-e84d1ad24fdc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "plt.plot(pd.Series(new_rewards).rolling(20).mean(), color='blue', linewidth=2)\n",
        "plt.title(\"Smoothed Rewards (20-episode Moving Average)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward (Smoothed)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5ovqa97pIAZ",
        "outputId": "116dd1dc-d4d3-45f5-b17f-6d5be20791c0"
      },
      "outputs": [],
      "source": [
        "# Re-create agent with smaller LR\n",
        "agent = PPOAgent(\n",
        "    state_dim=len(data.columns)-1,\n",
        "    action_dim=4,\n",
        "    lr=5e-5      # ‚Üê NEW learning rate\n",
        ")\n",
        "\n",
        "# Load previous trained model (4000 eps)\n",
        "model_path = \"ppo_traffic_model_3500.pt\"\n",
        "agent.load_state_dict(torch.load(model_path))\n",
        "print(\"‚úî Loaded model with new LR for fine-tuning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qabrjxwCpQ5G",
        "outputId": "6f954d1b-8609-4b3c-c5cf-9cde85bb10f5"
      },
      "outputs": [],
      "source": [
        "print(\"Current learning rate:\")\n",
        "for g in agent.optimizer.param_groups:\n",
        "    print(g['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqY7cgrxpZWb",
        "outputId": "1a39cb17-0833-46b3-c564-9ebd014a6178"
      },
      "outputs": [],
      "source": [
        "new_rewards = train_ppo_next_phase(episodes=500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ui_nsip1N2"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.state_dict(), \"ppo_traffic_model_4500.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTZOKaBwpzqm"
      },
      "outputs": [],
      "source": [
        "print(\"Models saved: ppo_traffic_model_latest.pt, reward_log.json, ppo_traffic_model_4500.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
